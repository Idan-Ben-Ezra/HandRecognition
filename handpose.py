# -*- coding: utf-8 -*-
"""HandPose.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10uUU3yd-r35WyKQ_Rx798cRMB5fOVgDt

#This notebook reimplmemnts an existing project using our code, step by step
####Step One: changes from original - We use our dataset instead of theirs, and we dont use validation splits(yet). We also use 64*64 heatmaps instead of 128*128
####(we don't use normalization of the dataset)
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %autoreload 2

#!pip install utils

"""##Utils

###Prep utils
"""

import numpy as np
import matplotlib.pyplot as plt
import cv2
import torch
import torch.nn as nn
from tqdm import tqdm

N_KEYPOINTS = 21
N_IMG_CHANNELS = 3
RAW_IMG_SIZE = 224
MODEL_IMG_SIZE = 128
DATASET_MEANS = [0.3950, 0.4323, 0.2954]
DATASET_STDS = [0.1966, 0.1734, 0.1836]
MODEL_NEURONS = 16


COLORMAP = {
    "thumb": {"ids": [0, 1, 2, 3, 4], "color": "g"},
    "index": {"ids": [0, 5, 6, 7, 8], "color": "c"},
    "middle": {"ids": [0, 9, 10, 11, 12], "color": "b"},
    "ring": {"ids": [0, 13, 14, 15, 16], "color": "m"},
    "little": {"ids": [0, 17, 18, 19, 20], "color": "r"},
}


def projectPoints(xyz, K):
    """
    Projects 3D coordinates into image space.
    Function taken from https://github.com/lmb-freiburg/freihand
    """
    xyz = np.array(xyz)
    K = np.array(K)
    uv = np.matmul(K, xyz.T).T
    return uv[:, :2] / uv[:, -1:]


def get_norm_params(dataloader):
    """
    Calculates image normalization parameters.
    Mean and Std are calculated for each channel separately.

    Borrowed from this StackOverflow discussion:
    https://stackoverflow.com/questions/60101240/finding-mean-and-standard-deviation-across-image-channels-pytorch
    """
    mean = 0.0
    std = 0.0
    nb_samples = 0.0

    for i, sample in tqdm(enumerate(dataloader)):
        data = sample["image_raw"]
        batch_samples = data.size(0)
        data = data.view(batch_samples, data.size(1), -1)
        mean += data.mean(2).sum(0)
        std += data.std(2).sum(0)
        nb_samples += batch_samples

    mean /= nb_samples
    std /= nb_samples
    return {"mean": mean, "std": std}


def vector_to_heatmaps(keypoints):
    """
    Creates 2D heatmaps from keypoint locations for a single image
    Input: array of size N_KEYPOINTS x 2
    Output: array of size N_KEYPOINTS x MODEL_IMG_SIZE x MODEL_IMG_SIZE
    """
    heatmaps = np.zeros([N_KEYPOINTS, MODEL_IMG_SIZE, MODEL_IMG_SIZE])
    for k, (x, y) in enumerate(keypoints):
        x, y = int(x * MODEL_IMG_SIZE), int(y * MODEL_IMG_SIZE)
        if (0 <= x < MODEL_IMG_SIZE) and (0 <= y < MODEL_IMG_SIZE):
            heatmaps[k, int(y), int(x)] = 1

    heatmaps = blur_heatmaps(heatmaps)
    return heatmaps


def blur_heatmaps(heatmaps):
    """Blurs heatmaps using GaussinaBlur of defined size"""
    heatmaps_blurred = heatmaps.copy()
    for k in range(len(heatmaps)):
        if heatmaps_blurred[k].max() == 1:
            heatmaps_blurred[k] = cv2.GaussianBlur(heatmaps[k], (51, 51), 3)
            heatmaps_blurred[k] = heatmaps_blurred[k] / heatmaps_blurred[k].max()
    return heatmaps_blurred


class IoULoss(nn.Module):
    """
    Intersection over Union Loss.
    IoU = Area of Overlap / Area of Union
    IoU loss is modified to use for heatmaps.
    """

    def __init__(self):
        super(IoULoss, self).__init__()
        self.EPSILON = 1e-6

    def _op_sum(self, x):
        return x.sum(-1).sum(-1)

    def forward(self, y_pred, y_true):
        inter = self._op_sum(y_true * y_pred)
        union = (
            self._op_sum(y_true ** 2)
            + self._op_sum(y_pred ** 2)
            - self._op_sum(y_true * y_pred)
        )
        iou = (inter + self.EPSILON) / (union + self.EPSILON)
        iou = torch.mean(iou)
        return 1 - iou


def heatmaps_to_coordinates(heatmaps):
    """
    Heatmaps is a numpy array
    Its size - (batch_size, n_keypoints, img_size, img_size)
    """
    batch_size = heatmaps.shape[0]
    sums = heatmaps.sum(axis=-1).sum(axis=-1)
    sums = np.expand_dims(sums, [2, 3])
    normalized = heatmaps / sums
    x_prob = normalized.sum(axis=2)
    y_prob = normalized.sum(axis=3)

    arr = np.tile(np.float32(np.arange(0, 64)), [batch_size, 21, 1])
    x = (arr * x_prob).sum(axis=2)
    y = (arr * y_prob).sum(axis=2)
    keypoints = np.stack([x, y], axis=-1)
    return keypoints / 64


def show_data(dataset, n_samples=12):
    """
    Function to visualize data
    Input: torch.utils.data.Dataset
    """
    n_cols = 4
    n_rows = int(np.ceil(n_samples / n_cols))
    plt.figure(figsize=[15, n_rows * 4])

    ids = np.random.choice(dataset.__len__(), n_samples, replace=False)
    for i, id_ in enumerate(ids, 1):
        sample = dataset.__getitem__(id_)

        # needs to be changed
        image, labels = sample
        image = image.numpy()
        image *= 255 # reversing the transformations applied on the image
        image = cv2.resize(image, (RAW_IMG_SIZE, RAW_IMG_SIZE))
        labels *= RAW_IMAGE_SIZE
        
        '''
        image = sample["image_raw"].numpy()
        image = np.moveaxis(image, 0, -1)
        keypoints = sample["keypoints"].numpy()
        keypoints = keypoints * RAW_IMG_SIZE'''


        plt.subplot(n_rows, n_cols, i)
        plt.imshow(image)
        plt.scatter(keypoints[:, 0], keypoints[:, 1], c="k", alpha=0.5)
        for finger, params in COLORMAP.items():
            plt.plot(
                keypoints[params["ids"], 0],
                keypoints[params["ids"], 1],
                params["color"],
            )
    plt.tight_layout()
    plt.show()


def show_batch_predictions(batch_data, model):
    """
    Visualizes image, image with actual keypoints and
    image with predicted keypoints.
    Finger colors are in COLORMAP.

    Inputs:
    - batch data is batch from dataloader
    - model is trained model
    """
    inputs = batch_data["image"]
    true_keypoints = batch_data["keypoints"].numpy()
    batch_size = true_keypoints.shape[0]
    pred_heatmaps = model(inputs)
    pred_heatmaps = pred_heatmaps.detach().numpy()
    pred_keypoints = heatmaps_to_coordinates(pred_heatmaps)
    images = batch_data["image_raw"].numpy()
    images = np.moveaxis(images, 1, -1)

    plt.figure(figsize=[12, 4 * batch_size])
    for i in range(batch_size):
        image = images[i]
        true_keypoints_img = true_keypoints[i] * RAW_IMG_SIZE
        pred_keypoints_img = pred_keypoints[i] * RAW_IMG_SIZE

        plt.subplot(batch_size, 3, i * 3 + 1)
        plt.imshow(image)
        plt.title("Image")
        plt.axis("off")

        plt.subplot(batch_size, 3, i * 3 + 2)
        plt.imshow(image)
        for finger, params in COLORMAP.items():
            plt.plot(
                true_keypoints_img[params["ids"], 0],
                true_keypoints_img[params["ids"], 1],
                params["color"],
            )
        plt.title("True Keypoints")
        plt.axis("off")

        plt.subplot(batch_size, 3, i * 3 + 3)
        plt.imshow(image)
        for finger, params in COLORMAP.items():
            plt.plot(
                pred_keypoints_img[params["ids"], 0],
                pred_keypoints_img[params["ids"], 1],
                params["color"],
            )
        plt.title("Pred Keypoints")
        plt.axis("off")
    plt.tight_layout()

"""##Dataset

"""

import numpy as np
import os
from PIL import Image
import json
import torch
from torchvision import transforms
from torch.utils.data import Dataset
'''
from utils.prep_utils import (
    projectPoints,
    vector_to_heatmaps,
    RAW_IMG_SIZE,
    MODEL_IMG_SIZE,
    DATASET_MEANS,
    DATASET_STDS,
)'''


class FreiHAND(Dataset):
    """
    Class to load FreiHAND dataset. Only training part is used here.
    Augmented images are not used, only raw - first 32,560 images

    Link to dataset:
    https://lmb.informatik.uni-freiburg.de/resources/datasets/FreihandDataset.en.html
    """

    def __init__(self, config, set_type="train"):
        self.device = config["device"]
        self.image_dir = os.path.join(config["data_dir"], "training/rgb")
        self.image_names = np.sort(os.listdir(self.image_dir))

        fn_K_matrix = os.path.join(config["data_dir"], "training_K.json")
        with open(fn_K_matrix, "r") as f:
            self.K_matrix = np.array(json.load(f))

        fn_anno = os.path.join(config["data_dir"], "training_xyz.json")
        with open(fn_anno, "r") as f:
            self.anno = np.array(json.load(f))

        if set_type == "train":
            n_start = 0
            n_end = 26000
        elif set_type == "val":
            n_start = 26000
            n_end = 31000
        else:
            n_start = 31000
            n_end = len(self.anno)

        self.image_names = self.image_names[n_start:n_end]
        self.K_matrix = self.K_matrix[n_start:n_end]
        self.anno = self.anno[n_start:n_end]

        self.image_raw_transform = transforms.ToTensor()
        self.image_transform = transforms.Compose(
            [
                transforms.Resize(MODEL_IMG_SIZE),
                transforms.ToTensor()])
        ''',
                transforms.Normalize(mean=DATASET_MEANS, std=DATASET_STDS),
            ]
        )'''

    def __len__(self):
        return len(self.anno)

    def __getitem__(self, idx):
        image_name = self.image_names[idx]
        image_raw = Image.open(os.path.join(self.image_dir, image_name))
        image = self.image_transform(image_raw)
        image_raw = self.image_raw_transform(image_raw)
        
        keypoints = projectPoints(self.anno[idx], self.K_matrix[idx])
        keypoints = keypoints / RAW_IMG_SIZE
        heatmaps = vector_to_heatmaps(keypoints)
        keypoints = torch.from_numpy(keypoints)
        heatmaps = torch.from_numpy(np.float32(heatmaps))

        return {
            "image": image,
            "keypoints": keypoints,
            "heatmaps": heatmaps,
            "image_name": image_name,
            "image_raw": image_raw,
        }



"""##Our dataset """

import cv2
import torch
import numpy as np
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

import os
import csv


#from heatmaps import *

DATASET_ROOT_DIR_PATH = "D:\magshimim\project\dataset"
# DATASET_PATH = os.path.join(DATASET_ROOT_DIR_PATH, "Dataset")
DATASET_PATH = os.path.join(DATASET_ROOT_DIR_PATH, "Dataset/Dataset")
# DATASET_COPY_PATH = os.path.join(DATASET_ROOT_DIR_PATH, "DatasetCopy")
LABEL_PATH = os.path.join(DATASET_ROOT_DIR_PATH, "Data_Labels.csv")
FILE_ENDING = ".jpg"

IMAGE_HEIGHT = 128
IMAGE_WIDTH = 128


def extract_labels(filename):
  """

  :param filename:
  :return:
  """
  with open(filename, newline='') as f:
    reader = csv.reader(f)
    listed = list(reader)
  return listed

get_id_from_name = (lambda name : int(name.split(".")[0]))
get_name_from_id = (lambda id: str(id) + FILE_ENDING)

"""
Class containing the hand image Dataset 
"""

class Dataset(torch.utils.data.Dataset):

    def __init__(self, images_dir, labels_path, transform=(lambda x: x), label_transform=(lambda x: x)):
        """
    Class constructor

    """
        super(Dataset, self).__init__()
        self.labels = extract_labels(labels_path)
        self.images = sorted(os.listdir(
            images_dir), key=get_id_from_name)  # loading the image names this way takes a lot of time(not sure whether they have to be sorted)
        print("Last image:", self.images[-1])
        self.images_dir = images_dir
        self.transform = transform
        self.label_transform = label_transform

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()

        image_name = self.images[idx]
        image = cv2.imread(os.path.join(self.images_dir, image_name))
        image = self.transform(image)

        label = self.labels[idx]
        label = [float(i) for i in label]
        label = torch.tensor(label)
        label = self.label_transform(label)

        return image, label


"""
The transformations applied on the data:
"""

resize_transform = (lambda img : cv2.resize(img, (IMAGE_WIDTH, IMAGE_HEIGHT)))
scale_transform = (lambda img: np.cast['float32'](img) / 255.0)# the pixels in the image are scaled from 0-255 to 0-1)
# a normalization transformation can also be added

HEATMAP_SIZE = 64

def get_heatmaps(key_points):
    """
    Creates 2D heatmaps from keypoint locations for a single image
    Input: array of size N_KEYPOINTS x 2 + 1
    Output: array of size N_KEYPOINTS x MODEL_IMG_SIZE x MODEL_IMG_SIZE
    """
    xkp = key_points[1::2]
    ykp = key_points[2::2]
    key_points = list(zip(xkp, ykp))
    heatmaps = np.zeros([N_KEYPOINTS, HEATMAP_SIZE, HEATMAP_SIZE])
    if key_points[0]:
        for k, (x, y) in enumerate(key_points):
            x, y = int(x * HEATMAP_SIZE), int(y * HEATMAP_SIZE)
            if (0 <= x < HEATMAP_SIZE) and (0 <= y < HEATMAP_SIZE):
                heatmaps[k, int(y), int(x)] = 1

        heatmaps = blur_heatmaps(heatmaps)
    return heatmaps

coordinate_to_heatmap_transform = get_heatmaps

def blur_heatmaps(heatmaps):
    """Blurs heatmaps using GaussinaBlur of defined size"""
    heatmaps_blurred = heatmaps.copy()
    for k in range(len(heatmaps)):
        if heatmaps_blurred[k].max() == 1:
            heatmaps_blurred[k] = cv2.GaussianBlur(heatmaps[k], (51, 51), 3)  # using the default values for now
            heatmaps_blurred[k] = heatmaps_blurred[k] / heatmaps_blurred[k].max()
    return heatmaps_blurred


"""
The transformations applied to the labels
"""
coordinate_to_heatmap_transform = get_heatmaps


"""####Dataset split"""

import shutil
import os
from tqdm import tqdm

# Commented out IPython magic to ensure Python compatibility.

"""##Model

"""

import torch
import torch.nn as nn

#from utils.prep_utils import MODEL_NEURONS


class ConvBlock(nn.Module):
    def __init__(self, in_depth, out_depth):
        super().__init__()
        self.double_conv = nn.Sequential(
            nn.BatchNorm2d(in_depth),
            nn.Conv2d(in_depth, out_depth, kernel_size=3, padding=1, bias=False),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(out_depth),
            nn.Conv2d(out_depth, out_depth, kernel_size=3, padding=1, bias=False),
            nn.ReLU(inplace=True),
        )

    def forward(self, x):
        return self.double_conv(x)


class ShallowUNet(nn.Module):
    """
    Implementation of UNet, slightly modified:
    - less downsampling blocks
    - less neurons in the layers
    - Batch Normalization added
    
    Link to paper on original UNet:
    https://arxiv.org/abs/1505.04597
    """
    
    def __init__(self, in_channel, out_channel):
        super().__init__()

        self.conv_down1 = ConvBlock(in_channel, MODEL_NEURONS)
        self.conv_down2 = ConvBlock(MODEL_NEURONS, MODEL_NEURONS * 2)
        self.conv_down3 = ConvBlock(MODEL_NEURONS * 2, MODEL_NEURONS * 4)
        self.conv_bottleneck = ConvBlock(MODEL_NEURONS * 4, MODEL_NEURONS * 8)

        self.maxpool = nn.MaxPool2d(2)
        self.upsamle = nn.Upsample(scale_factor=2, mode="bilinear", align_corners=False)

        self.conv_up1 = ConvBlock(
            MODEL_NEURONS * 8 + MODEL_NEURONS * 4, MODEL_NEURONS * 4
        )
        self.conv_up2 = ConvBlock(
            MODEL_NEURONS * 4 + MODEL_NEURONS * 2, MODEL_NEURONS * 2
        )
        self.conv_up3 = ConvBlock(MODEL_NEURONS * 2 + MODEL_NEURONS, MODEL_NEURONS)

        self.conv_out = nn.Sequential(
            nn.Conv2d(MODEL_NEURONS, out_channel, kernel_size=3, padding=1, bias=False),
            nn.Sigmoid(),
        )

    def forward(self, x):
        #print("X - ", x.shape)
        conv_d1 = self.conv_down1(x)
        #print("Conv down 1 - ", conv_d1.shape)
        conv_d2 = self.conv_down2(self.maxpool(conv_d1))
        #print("Conv down 2 - ", conv_d2.shape)
        conv_d3 = self.conv_down3(self.maxpool(conv_d2))
        #print("Conv down 3 - ", conv_d3.shape)
        conv_b = self.conv_bottleneck(self.maxpool(conv_d3))
        #print("Conv bottleneck - ", conv_b.shape)

        conv_u1 = self.conv_up1(torch.cat([self.upsamle(conv_b), conv_d3], dim=1))
        #print("Conv up 1 - ", conv_u1.shape)
        conv_u2 = self.conv_up2(torch.cat([self.upsamle(conv_u1), conv_d2], dim=1))
        #print("Conv up 2 - ", conv_u2.shape)
        conv_u3 = self.conv_up3(torch.cat([self.upsamle(conv_u2), conv_d1], dim=1))
        #print("Conv up 3 - ", conv_u3.shape)

        out = self.conv_out(self.maxpool(conv_u3))  # should be u3, originally maxpool isn't used
        #print("out - ", out.shape)
        return out

"""##Our Model -"""

import os
import torch
import numpy as np
import cv2
import csv
import matplotlib.pyplot as plt
from tqdm import tqdm
import torchvision
import sys
#DATASET_ROOT_DIR_PATH = "/content/drive/My Drive/Dataset"
DATASET_ROOT_DIR_PATH = "D:\magshimim\project\dataset"
torch.manual_seed(1)

"""##Consts"""

INPUT_HEIGHT = 128
INPUT_WIDTH = 128
RGB_CHANNELS = 3
HEATMAP_CHANNELS = 21
STRIDE = 2
DECONV_STRIDE = 2
DECONV_KERNEL_SIZE = 2
MAX_POOL_FILTER = (2,2)

HOURGLASS_OUTPUT_CHANNELS = 128
#HOURGLASS_OUTPUT_CHANNELS = 64
UPSCALE = 2
BATCH_NORM_MOMENTUM = 1e-5

# after every applied Conv filter- the image shrinks,
# if the images dimensions were (C, H, W), and padding=p, stride=s, kernel_size=f
# then the output would be - Hout = lower((H+2*p-f)/s + 1)
calc_out_size = (lambda h,p,s,f : int((h+2*p-f)/s + 1))


DATASET_PATH = os.path.join(DATASET_ROOT_DIR_PATH, "Dataset")
LABEL_PATH = os.path.join(DATASET_ROOT_DIR_PATH, "Data_Labels.csv")
DEFAULT_FILENAME = "model.bin"

# Loss
HAND_DETECTION_INDEX = 0  # besides the keypoint coordinates, the Model returns a bool representing whether a hand was detected in the
# image, it's index in the labels is 0
INVALID_DETECTION_LOSS_VALUE = 1  # if the model detects a hand incorrectly, the loss value should is 1, ignoring the actual difference


# HMs to KP
HEATMAP_SIZE = 64
BLUR_HYPER_PARAMETER = 5
BLUR_GRID = (BLUR_HYPER_PARAMETER, BLUR_HYPER_PARAMETER)
MAX_VALUE = 1
MIN_VALUE = 0

N_KEYPOINTS = 21


# Conv Block

KERNEL_SIZE = 7
PADDING = 3  # if we want the output's height to be half of the input's height(while the kernel_size is 7),
#  we need the padding to be 3 - based on https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html
SAME_PADDING = 67


# Sizes

IMAGE_HEIGHT = 128
IMAGE_WIDTH = 128

# Dataset

def resize_transform(img):
    return cv2.resize(img, (IMAGE_WIDTH, IMAGE_HEIGHT))

def scale_transform(img):
    return np.cast['float32'](img) / 255.0  # the pixels in the image are scaled from 0-255 to 0-1)
# a normalization transformation can also be added


# Model

HOURGLASS_LAYERS = 1

# Training
CURR_EPOCH = 1
EPOCHS = 100
BATCH_SIZE = 1
INIT_LEARNING_RATE = 1e-3
WEIGHT_DECAY = 0

TRAIN_RATIO= 0.8
VALIDATION_RATIO= 0.2
DATASET_LENGTH = 35971

IMG_NAME="1.jpg"
IMG = os.path.join(DATASET_PATH, IMG_NAME)
# LOSS

class get_heatmap_loss(torch.nn.Module):

    def __init__(self):
        super(get_heatmap_loss, self).__init__()

    def forward(self, x, y):
        total_loss = 0
        if not isinstance(x, list):
            x = [x]
        for output in x:
            assert(output.shape == y.shape)
            z = (output - y).float()
            mse_mask = (torch.abs(z) < 0.01).float()
            l1_mask = (torch.abs(z) >= 0.01).float()
            mse = mse_mask * z
            l1 = l1_mask * z
            total_loss += torch.mean(self._calculate_MSE(mse)*mse_mask)
            total_loss += torch.mean(self._calculate_L1(l1)*l1_mask)

        return total_loss

    def _calculate_MSE(self, z):
        return 0.5 *(torch.pow(z, 2))

    def _calculate_L1(self, z):
        return 0.01 * (torch.abs(z) - 0.005)


def __joint_keypoint_loss(loss_func):
    """
    Decorator returns the function which calculates the loss
    :param loss_func: the loss function applied to the output
    :return: the inner function, which calculates the loss
    """

    def calculate_loss(outputs, labels):
        if outputs[HAND_DETECTION_INDEX] != labels[HAND_DETECTION_INDEX]:
            return INVALID_DETECTION_LOSS_VALUE
        return loss_func(outputs, labels)

    return calculate_loss


def get_joint_keypoint_loss():
    return __joint_keypoint_loss(torch.nn.SmoothL1Loss())

import scipy

def get_mass_center_loss():

  def calc_loss(model_output, y):
    """
    The loss is the absoulte difference between the real heatmaps center of mass,
        and the outputted heatmap's center of mass
    """
    y_center = torch.FloatTensor(scipy.ndimage.center_of_mass(y))
    model_center = torch.FloatTensor(scipy.ndimage.center_of_mass(model_output))

    return abs(y_center - model.center)

  return calc_loss


# HMs to KP


def get_heatmaps(key_points):
    """
    Creates 2D heatmaps from keypoint locations for a single image
    Input: array of size N_KEYPOINTS x 2 + 1
    Output: array of size N_KEYPOINTS x MODEL_IMG_SIZE x MODEL_IMG_SIZE
    """
    xkp = key_points[1::2]
    ykp = key_points[2::2]
    key_points = list(zip(xkp, ykp))
    heatmaps = np.zeros([N_KEYPOINTS, HEATMAP_SIZE, HEATMAP_SIZE])
    for k, (x, y) in enumerate(key_points):
        x, y = int(x * HEATMAP_SIZE), int(y * HEATMAP_SIZE)
        if (0 <= x < HEATMAP_SIZE) and (0 <= y < HEATMAP_SIZE):
            heatmaps[k, int(y), int(x)] = 1

    heatmaps = blur_heatmaps(heatmaps)
    return heatmaps

coordinate_to_heatmap_transform = get_heatmaps

def blur_heatmaps(heatmaps):
    """Blurs heatmaps using GaussinaBlur of defined size"""
    heatmaps_blurred = heatmaps.copy()
    for k in range(len(heatmaps)):
        if heatmaps_blurred[k].max() == 1:
            heatmaps_blurred[k] = cv2.GaussianBlur(heatmaps[k], (51, 51), 7)
            heatmaps_blurred[k] = heatmaps_blurred[k] / heatmaps_blurred[k].max()
    return heatmaps_blurred

'''def get_batch_heatmaps(labels):
    """
    Function returns the heatmaps for a given batch of labels
    """
    print(labels.shape, labels, len(labels.shape))
    if len(labels.shape) == 1:  # if the batch consits only of a single record
      print("su")
      return get_heatmaps(labels)
    return torch.FloatTensor([get_heatmaps(single_image_label) for single_image_label in labels])
'''


def parse_heatmaps(heatmaps):
    """
    Heatmaps is a numpy array
    Its size - (batch_size, n_keypoints, img_size, img_size)
    """
    batch_size = heatmaps.shape[0]
    sums = heatmaps.sum(axis=-1).sum(axis=-1)
    sums = np.expand_dims(sums, [2, 3])
    normalized = heatmaps / sums
    x_prob = normalized.sum(axis=2)
    y_prob = normalized.sum(axis=3)

    arr = np.tile(np.float32(np.arange(0, 64)), [batch_size, 21, 1])
    x = (arr * x_prob).sum(axis=2)
    y = (arr * y_prob).sum(axis=2)
    keypoints = np.stack([x, y], axis=-1)
    return keypoints / 64



#
# CLASSES
#

# Bottleneck


class Bottleneck(torch.nn.Module):
    """
    The Bottleneck block used in the Hourglass architecture
    The block contains 3 conv layers:
    a 1 by 1 layer, a 3 by 3 layer, and a 1 by 1 layer
    (batch normalization are added for increased efficiency)
    the layer is generally used to imporve efficiency:
    it's much faster-has less learnable parameters- than a simple 3 by 3 convolution
    """

    PADDING = 'same'  # each layer is padded so that the output is of same size as the input

    def __init__(self, in_channels, out_channels, to_expand_channels=False, to_downsample=False):
        '''
        Class ctor
        :param in_channels: The amount of channels that the input has
        :param out_channels: The amount of channels that the output has.
        :param to_expand_channels: Whether the output's channels should be the 'out_channels' value.
          if not, output's channels eqauls 'in_channels'
        :param to_downsample: Whether the output's height should be reduced by a scale of 2,
          if not, the output height is the same as the input height
        '''
        super(Bottleneck, self).__init__()
        self.conv1 = torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1,
                                     padding=Bottleneck.PADDING)
        self.conv2 = torch.nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3,
                                     padding=Bottleneck.PADDING)
        self.conv3 = torch.nn.Conv2d(in_channels=out_channels, out_channels=in_channels, kernel_size=1,
                                     padding=Bottleneck.PADDING)
        # iconv3 output channels should be the same as the un_channles, because of the sum operation with the residual layer(has in_channels)
        self.relu = torch.nn.ReLU()
        self.batch_normalization1 = torch.nn.BatchNorm2d(num_features=out_channels)
        self.batch_normalization2 = torch.nn.BatchNorm2d(num_features=out_channels)
        self.batch_normalization3 = torch.nn.BatchNorm2d(num_features=in_channels)

        self.to_expand_channels = to_expand_channels
        if self.to_expand_channels:
            self.__init_channel_expansion(in_channels, out_channels)

        self.to_downsample = to_downsample
        if self.to_downsample:
            self.__init_downsample(in_channels,
                                   in_channels)  # applied on the residual, the amount of channels shouldn't be changed

        self.in_c = in_channels
        self.out_c = out_channels

    def __init_channel_expansion(self, in_channels, out_channels):
        '''
        Auxiliary method initalizes the channels expansion block, which is responsible for increasing the amount of channels-
        is responsible for making the Bottleneck's output contain 'out_channels' amount of channels
        :return: None
        '''
        self.expand_conv = torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1,
                                           padding=Bottleneck.PADDING)
        self.expand_batch_normalization = torch.nn.BatchNorm2d(num_features=out_channels)

    def __init_downsample(self, in_channels, out_channels):
        '''
        Auxiliary method initalizes the dwonsampling layer, used for decreasing the output height of the Bottleneck
        is used by the Hourglass for residual layer
        :return: None
        '''
        self.downsample = torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, padding=0,
                                          stride=STRIDE)

    @staticmethod
    def get_out_height(in_height):
        '''
        Auxiliary method outputs the height of the bottleneck's output
        :param in_height: the input's height
        :return: the new output's height
        '''
        # height1 = calc_out_size(in_height, )
        # note that in this implementation the bottleneck keeps the output the same size
        return in_height

    def __call__(self, x):
        return self.forward(x)

    def forward(self, x):
        # print("Channels: ", self.in_c, self.out_c)
        residual = x
        # print("X - ", x.shape)
        y = self.relu(self.conv1(x))
        y = self.batch_normalization1(y)
        # print("Y1 - ", y.shape)
        y = self.relu(self.conv2(y))
        y = self.batch_normalization2(y)
        # print("Y2 - ",y.shape)
        y = self.conv3(y)
        y = self.batch_normalization3(y)
        # print("Y3 - ",y.shape)
        # the bottleneck block performs a skip connection from the input to the last layer

        y += residual

        if self.to_downsample:  # decreasing the height of the output, so that it fits the next layer
            y = self.downsample(y)

        if self.to_expand_channels:  # expanding the amount of channels outputted by the bottleneck,
            # so that the output is of depth 'out_channels'
            y = self.expand_conv(y)
            y = self.expand_batch_normalization(y)

        y = self.relu(y)

        return y


# Connector


class Connector(torch.nn.Module):
    """
    This layer connects stacked Hourglass networks together -
    In the model architecture, the backbone network consists of multiple hourglass models stacked on each other.
    This layer converts the output of one hourglass into the input of the next horuglass
    """

    def __init__(self, in_channels=HOURGLASS_OUTPUT_CHANNELS, out_channels=RGB_CHANNELS,
                 heatmap_channels=HEATMAP_CHANNELS):
        '''
        class ctor, note that it's input is the hourglass output,
        while it's output is the next hourglass' input
        :param in_channels: The amount of channels in the hourglass' output
        :param out_channels: THe amount of channels a hourglass receives as input
        '''
        super(Connector, self).__init__()
        self.conv1 = torch.nn.Conv2d(in_channels=in_channels, out_channels=heatmap_channels, kernel_size=1)

        self.conv2 = torch.nn.Conv2d(in_channels=heatmap_channels, out_channels=out_channels, kernel_size=1)

        self.conv3 = torch.nn.Conv2d(in_channels=heatmap_channels, out_channels=out_channels, kernel_size=1)
        # the conv applied after loss is calculated on heatmaps

        self.deconv_layer = torch.nn.ConvTranspose2d(in_channels=out_channels, out_channels=out_channels,
                                                     kernel_size=DECONV_KERNEL_SIZE,
                                                     stride=DECONV_STRIDE)  # the layer upscales the input so that the next horuglass
        # receives a 128*128 sized input(instead of a 64*64 sized heatmap)

    def __call__(self, x, network_input):
        return self.forward(x, network_input)

    def forward(self, x, network_input):
        '''
        The forward method of this layer
        :param x: the output of the previous hourglass
        :param network_input: the input of the previous hourglass
        :return: a tuple containing: the input to the next hourglass,
            the heatmaps on which a loss is going to be calculated(losses are calculated on each hourglass' output separately)
        '''
        # print("X - ", x.shape, "net inp - ", network_input.shape)
        x = self.conv1(x)
        heatmap_preds = x  # the heatmap predictions on which a loss is going to be calculated
        # print("hm - ", heatmap_preds.shape)
        y = self.conv2(x)
        residual_y = self.conv3(heatmap_preds)
        # print("Y - ", y.shape, "res y - ", residual_y.shape)

        y = y + residual_y
        upscaled_y = self.deconv_layer(
            y)  # the output is upscaled so that it aligns with the next hourglass' dimensionality
        upscaled_y += network_input
        # print("Finished connecting")

        return upscaled_y, heatmap_preds


# Conv Block


class ConvBlock(torch.nn.Module):
    """
  The basic Conv block that is used by the horuglass model
  The block itself contains a 7*7 conv layer, a batch normalization layer and a Relu activation function
  """

    def __init__(self, in_channels, out_channels, kernel_size=KERNEL_SIZE, stride=STRIDE, padding=PADDING):
        super(ConvBlock, self).__init__()
        self.conv_layer = torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,
                                          stride=stride, padding=SAME_PADDING)
        self.bn_layer = torch.nn.BatchNorm2d(num_features=out_channels, momentum=BATCH_NORM_MOMENTUM)
        self.relu = torch.nn.ReLU()

    def __call__(self, x):
        return self.forward(x)

    def forward(self, x):
        x = self.conv_layer(x)
        x = self.bn_layer(x)
        x = self.relu(x)

        return x


# Dataset

def extract_labels(filename):
    """

  :param filename:
  :return:
  """
    with open(filename, newline='') as f:
        reader = csv.reader(f)
        listed = list(reader)
    return listed

FILE_ENDING = ".jpg"
get_id_from_name = (lambda name : int(name.split(".")[0]))
get_name_from_id = (lambda id: str(id) + FILE_ENDING)

class Dataset(torch.utils.data.Dataset):

    def __init__(self, images_dir, labels_path, transform=(lambda x: x), label_transform=(lambda x: x)):
        """
    Class constructor

    """
        super(Dataset, self).__init__()
        self.labels = extract_labels(labels_path)
        print(images_dir)
        self.images = sorted(os.listdir(
            images_dir), key=get_id_from_name)  # loading the image names this way takes a lot of time(not sure whether they have to be sorted)
        print("Last image:", self.images[-1])
        self.images_dir = images_dir
        self.transform = transform
        self.label_transform = label_transform

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()

        image_name = self.images[idx]
        image = cv2.imread(os.path.join(self.images_dir, image_name))
        image = self.transform(image)

        label = self.labels[idx]
        label = [float(i) for i in label]
        label = torch.tensor(label)
        label = self.label_transform(label)

        return image, label


# Loader

class Loader:
    def __init__(self, filename='checkpoint.pth'):
        self._filename = filename

    def save_checkpoint(self, model, optimizer, lr_sched, epoch=None):
        checkpoint = {
      'epoch': epoch,
      'model': model.state_dict(),
      'optimizer': optimizer.state_dict(),
      'lr_sched': lr_sched}
        torch.save(checkpoint, self._filename)
      
    def save(self,model):
        torch.save(model.state_dict(), self._filename)

    def load_checkpoint(self):
        checkpoint = torch.load(self._filename)
        model = Model()
        print(model.load_state_dict(checkpoint['model']))        
        model = model.cuda() #  model should be moved to gpu before optimizers are created - 

        optimizer = torch.optim.Adam(model.parameters(), lr=INIT_LEARNING_RATE, weight_decay=WEIGHT_DECAY)
        optimizer.load_state_dict(checkpoint['optimizer'])
        
        '''scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',patience=5,factor=0.5,min_lr=1e-6,verbose=True)
        scheduler.load_state_dict(checkpoint['lr_sched'])
        checkpoint['lr_sched'] = scheduler'''

        try:  # this code throws an exception if optimizer.step() hasn't yet been called, which initializes the 
            #  lr_scheduler._last_lr member
          for g in optimizer.param_groups:
            g['lr'] = checkpoint['lr_sched']._last_lr[0]
        except:
          pass
        checkpoint['model'] = model
        checkpoint['optimizer'] = optimizer

        return checkpoint

    def load(self):
        model = Model()
        model.load_state_dict(torch.load(self._filename))
        return model

# Model


class DecoderBlock(torch.nn.Module):
    """
  The basic block in the decoder part of the hourglass.
  Contains 1)an upscaling of the previous layer's input,
  and 2)a bottleneck that receives as input a residual value.
  The two are added together.
  """

    def __init__(self, in_dimensions, out_dimensions, kernel_size=DECONV_KERNEL_SIZE):
        '''
    class constructor
    :param in_dimensions: the dimensions of the given input, in the format: [N, Channels, Height, Width]
    :param out_dimension: the dimensions of the expected input, in the format: [N, Channels, Height, Width]
    '''
        super(DecoderBlock, self).__init__()
        assert len(in_dimensions) == 4 and len(
            out_dimensions) == 4  # checking that the batch size is part of the input dimensions

        self.deconv_layer = torch.nn.ConvTranspose2d(in_channels=in_dimensions[1], out_channels=out_dimensions[1],
                                                     kernel_size=kernel_size,
                                                     stride=DECONV_STRIDE)  # the deconv layer allows us to increase the feature's hieght amd width(while also learning new weights)
        self.bn_layer = torch.nn.BatchNorm2d(num_features=out_dimensions[1], momentum=BATCH_NORM_MOMENTUM)
        self.relu = torch.nn.ReLU()
        self.bottleneck = Bottleneck(in_channels=out_dimensions[1], out_channels=out_dimensions[1],
                                     to_expand_channels=True)
        self.in_c = in_dimensions[1]
        self.out_c = out_dimensions[1]

    def __call__(self, x, residual):
        return self.forward(x, residual)

    def forward(self, x, residual):
        '''
    :param x: the output of the previous layer
    :param residual: the residual output fed to this layer
    '''
        # print("IN channels: ", self.in_c, "OUT channels: ", self.out_c)
        # print("Decode: X - ", x.shape, "res - ", residual.shape)
        x = self.deconv_layer(x)
        # print("Decode: X - ", x.shape, "res - ", residual.shape)
        x = self.relu(self.bn_layer(x))
        # print("Decode: X - ", x.shape, "res - ", residual.shape)
        residual = self.bottleneck(residual)
        # print("Decode: X - ", x.shape, "res - ", residual.shape)
        y = x + residual

        return y


class EncoderBlock(torch.nn.Module):
    """
  The basic block in the decoder part of the hourglass.
  Contains 1)an upscaling of the previous layer's input,
  and 2)a bottleneck that receives as input a residual value.
  The two are added together.
  """

    def __init__(self, in_channels, out_channels):
        '''
    class constructor
    :param in_channels: The amount of channels that the input has
    :param out_channels: The amount of channels that the output has.
    '''
        super(EncoderBlock, self).__init__()
        assert in_channels * 2 == out_channels  # the depth of the features is increased times 2 by each block
        self.bottleneck = Bottleneck(in_channels=in_channels, out_channels=out_channels, to_expand_channels=True)
        self.max_pool = torch.nn.MaxPool2d(MAX_POOL_FILTER, stride=STRIDE, padding=0, dilation=1, return_indices=False,
                                           ceil_mode=False)

    @staticmethod
    def get_out_height(in_height):
        '''
    Auxiliary method outputs the height of the layer's final output
    :param in_height: the input's height
    :return: the new output's height
    '''
        height1 = int(in_height / 2)  # the max pool divides the height by 2
        height2 = Bottleneck.get_out_height(height1)

        return height2

    def __call__(self, x):
        return self.forward(x)

    def forward(self, x):
        '''
    :return: a tuple containing: the input to the encoding layer,
            the input to the residual layer connected to the encoder
    '''
        y_residual = self.bottleneck(x)  # this output is used by the residual layers
        y = self.max_pool(y_residual)  # this output is use by the next encoding layer

        return y, y_residual


class FeatureExtractor(torch.nn.Module):
    """
    This layer extracts the deepest features in the Hourglass network, its features are the smallets and in height, and
    biggest in deoth.   After this layer the features are decoded into a heatmap.
    """

    def __init__(self, in_channels, out_channels):
        '''
        class ctor
        :param in_channels: The amount of channels that the input has. If input is of shape [N, C, H, W]- the second dim= channels
        :param out_channels: The amount of channels that the output has.
        '''
        super(FeatureExtractor, self).__init__()
        self.bottleneck1 = Bottleneck(in_channels=in_channels, out_channels=in_channels * 2, to_expand_channels=True)
        # need to check whether the in_channles, out_channels value is correct
        self.bottleneck2 = Bottleneck(in_channels=in_channels * 2, out_channels=in_channels * 2)
        # need to check whether the in_channles, out_channels value is correct
        self.bottleneck3 = Bottleneck(in_channels=in_channels * 2, out_channels=in_channels * 2,
                                      to_expand_channels=True)
        # need to check whether the in_channles, out_channels value is correct

        self.upsample = torch.nn.Upsample(scale_factor=UPSCALE)
        # A upsample is used to increase the extracted features' height so that it matches the reidual feature's height
        # there is a difference in height because a max pool layer is applied by the decoder only after the residual connection

        self.residual_max_pool = torch.nn.MaxPool2d(MAX_POOL_FILTER, stride=STRIDE, padding=0, dilation=1,
                                                    return_indices=False, ceil_mode=False)
        # A max pool is used to reduce the reidual feature's height so that it matches the features' height
        # there is a difference in height because a max pool layer is applied by the decoder only after the residual connection

    def __call__(self, x, residual_y):
        return self.forward(x, residual_y)

    def forward(self, x, residual_y):
        '''
        The forward method of the final feature extraction layer
        :param x: the output of the previous layer
        :param residual_y: the residual output (after it has been passed through a bottleneck)
        :return: the final extracted feature(the most high-level features)
        '''
        # print("extractor: ", x.shape, residual_y.shape)
        y = self.bottleneck1(x)
        # print("extractor: ", y.shape, residual_y.shape)
        y = self.bottleneck2(y)
        y = self.bottleneck3(y)
        # print("extractor: ", y.shape, residual_y.shape)

        residual_y = self.residual_max_pool(
            residual_y)  # decreasing the residual's height so that it matches y's height
        # y = self.upsample(y)
        # print("shape ", residual_y.shape, y.shape)
        y = y + residual_y

        return y


class FirstLayer(torch.nn.Module):
    """
    The first layer on the architecture
    """

    def __init__(self, out_channels, in_channels=RGB_CHANNELS):
        '''
        class ctor
        :param in_channels: The amount of channels that the input has
        :param out_channels: The amount of channels that the output has.
        '''
        super(FirstLayer, self).__init__()
        self.conv_block = ConvBlock(in_channels=in_channels, out_channels=out_channels)
        self.bottleneck = Bottleneck(in_channels=out_channels, out_channels=out_channels)

        self.max_pool = torch.nn.MaxPool2d(MAX_POOL_FILTER, stride=STRIDE, padding=0, dilation=1, return_indices=False,
                                           ceil_mode=False)

    @staticmethod
    def get_out_height(in_height):
        '''
        Auxiliary method outputs the height of the layer's final output
        :param in_height: the input's height
        :return: the new output's height
        '''
        height1 = calc_out_size(in_height, 0, STRIDE, KERNEL_SIZE)
        height2 = Bottleneck.get_out_height(height1)

        return height2

    def forward(self, x):
        x = self.conv_block(x)
        y_residual = self.bottleneck(x)

        y = self.max_pool(y_residual)

        return y, y_residual


class Hourglass(torch.nn.Module):
    """
    The Backbone Hourglas-'Encoder-Decoder' architecture, based on the paper - https://arxiv.org/pdf/1603.06937.pdf.
    The network is based upon extensive use of Residual Bottleneck layers, which are used for both encoding and decoding features.
    First the image is encoded into features, which are then decoded(using residual connections to the encoded layers) into a heatmap.
    A more in depth explanation- https://towardsdatascience.com/using-hourglass-networks-to-understand-human-poses-1e40e349fa15
    """

    def __init__(self, in_channels=RGB_CHANNELS, out_channels=HEATMAP_CHANNELS, extract_heatmaps=False):
        '''
        Class ctor for hourglass network
        :param in_channels: the amount of channels(depth) of the input to the network, default value is 3-RGB image
        :param out_channels: the amount of channels(depth) of the network - only if it outputs heatmaps directly,
        meaning that it's the last layer - , is equal to the amount of heatmaps outputted, which is the amount of keypoints
        :param extract_heatmaps: bool value representing whether the network outputs heatmaps directly, basically
        representing whether the netowrk is the last hourglass in the backbone
        '''
        super(Hourglass, self).__init__()
        # the encoding layers of the model
        self.encoder_layer1 = FirstLayer(in_channels=in_channels,
                                         out_channels=64)  # the first layer foesn;t have a max pooling layer,
        # instead the dimensions are reduced by the Conv operation(which isn't padded) *unsure whether it;s the same as the paper
        # self.height1 = FirstLayer.get_out_height(INPUT_HEIGHT)  # h = 64
        self.encoder_layer2 = EncoderBlock(in_channels=64, out_channels=128)
        # self.height2 = EncoderBlock.get_out_height(self.height1) # 32
        self.encoder_layer3 = EncoderBlock(in_channels=128, out_channels=256)
        # self.height3 = EncoderBlock.get_out_height(self.height2) # 16
        self.encoder_layer4 = EncoderBlock(in_channels=256, out_channels=512)
        # self.height4 = EncoderBlock.get_out_height(self.height3) # 8
        self.feature_extractor = FeatureExtractor(in_channels=512,
                                                  out_channels=1024)  # out_channels may need to be changed to a different value

        # the residual layers of the model
        # it's unclear whether a residual layer consists a single or multiple bottlenecks

        # the residual layer's Height is always bigger than the corresponding decoder layer height, as during encoding,
        # the residual is outputted before the input goes through Max pooling, meaning that the residual's height isn't decreased
        # (while the feature's height is).
        # This is solved by downsampling the output in the residual layer itself, so that it height matches the decoder
        self.residual1 = Bottleneck(in_channels=64, out_channels=128, to_expand_channels=True, to_downsample=True)
        self.residual2 = Bottleneck(in_channels=128, out_channels=256, to_expand_channels=True, to_downsample=True)
        self.residual3 = Bottleneck(in_channels=256, out_channels=512, to_expand_channels=True, to_downsample=True)

        self.residual4 = Bottleneck(in_channels=512, out_channels=1024,
                                    to_expand_channels=True)  # this residual's output is sent to the feature extractor

        # after every applied Conv filter- the image shrinks,
        # if the images dimensions were (C, H, W), and padding=p, stride=s, kernel_size=f
        # then the output would be - Hout = lower((H+2*p-f)/s + 1)
        # the decoding layers of the network
        '''
        self.decoder_layer1 = DecoderBlock((BATCH_SIZE, 512, self.height3, self.height3), (BATCH_SIZE, 256, self.height2, self.height2)) # height-16*2= 32
        self.decoder_layer2 = DecoderBlock((BATCH_SIZE, 256, self.height3, self.height3), (BATCH_SIZE, 128, self.height2, self.height2)) # height-32*2= 64
        self.decoder_layer3 = DecoderBlock((BATCH_SIZE, 128, self.height4, self.height4), (BATCH_SIZE, 64, self.height3, self.height3))'''
        self.decoder_layer1 = DecoderBlock((BATCH_SIZE, 1024, 1, 1), (BATCH_SIZE, 512, 1, 1))  # height-8*2 =16
        self.decoder_layer2 = DecoderBlock((BATCH_SIZE, 512, 1, 1), (BATCH_SIZE, 256, 1, 1))  # height-16*2= 32
        self.decoder_layer3 = DecoderBlock((BATCH_SIZE, 256, 1, 1), (BATCH_SIZE, 128, 1, 1))  # height-32*2= 64

        self.heatmap_extractor = None
        if extract_heatmaps:
            self.heatmap_extractor = torch.nn.Conv2d(in_channels=128, out_channels=out_channels, kernel_size=1)
        # there might need to be an additional layer which converts the output into 42 heatmaps, or that layer
        # is inside the connector_layer(on which's output the loss is calculated for each hourglass)

        # whether the channels=HEATMAP_CHANNELS assignment is correct needs to be checked
        # print(self.height1, self.height2, self.height3, self.height4)

    def __call__(self, x):
        return self.forward(x)

    def forward(self, x):

        # the input is passed through each decoder layer,
        # and the output of each decoder is also passed to the residual
        layer1, res1 = self.encoder_layer1(x)
        res1 = self.residual1(res1)
        # the encoding blocks have 2 outputs: one for the next encoding layer and one for the residual layer
        layer2, res2 = self.encoder_layer2(layer1)
        res2 = self.residual2(res2)
        layer3, res3 = self.encoder_layer3(layer2)
        res3 = self.residual3(res3)
        layer4, res4 = self.encoder_layer4(layer3)
        res4 = self.residual4(res4)
        # the deepest res and deepest features are passed to the final feature extractor
        features = self.feature_extractor(layer4, res4)

        # print("Heights: ", res1.shape[-1], res2.shape[-1], res3.shape[-1], res4.shape[-1])
        # print("Heights: ", layer1.shape[-1], layer2.shape[-1], layer3.shape[-1], layer4.shape[-1])

        # print("Channels: ", layer1.shape[1],layer2.shape[1], layer3.shape[1], layer4.shape[1], features.shape[1] )
        # now the features and the corresponding residual are passed through the decoding layers
        # print("Deco1 input - ", features.shape, res3.shape)
        decoded1 = self.decoder_layer1(features, res3)
        decoded2 = self.decoder_layer2(decoded1, res2)
        final_decoded = self.decoder_layer3(decoded2, res1)

        # heatmaps = self.heatmap_extractor(decoded3)
        # it's unclear whether each hourglass outputs heatmaps(on which the loss is calculated),
        #  or whether the heatmaps are only created by the connector layer between two stacked hourglasses
        if self.heatmap_extractor:
            final_decoded = self.heatmap_extractor(final_decoded)

        return final_decoded


class Model(torch.nn.Module):
    """
    THe AWR model class

    """

    def __init__(self, num_hourglass=HOURGLASS_LAYERS):
        super(Model, self).__init__()
        self.hourglass_layers = torch.nn.ModuleList(
            [Hourglass(in_channels=RGB_CHANNELS, out_channels=HEATMAP_CHANNELS) for i in range(num_hourglass - 1)])
        # initializing all layers bu the last one, which extacts it's heatmaps directly (without a connector layer)
        self.hourglass_layers.append(
            Hourglass(in_channels=RGB_CHANNELS, out_channels=HEATMAP_CHANNELS, extract_heatmaps=True))
        self.connector_layers = torch.nn.ModuleList([Connector(in_channels=HOURGLASS_OUTPUT_CHANNELS,
                                                               out_channels=RGB_CHANNELS,
                                                               heatmap_channels=HEATMAP_CHANNELS)
                                                     for i in range(
                num_hourglass - 1)])  # there are n-1 connectors for n hourglass layers

    def __call__(self, x):
        return self.forward(x)

    def forward(self, x):
        hg_output = []
        # passing the input through all hourglass layers
        for i, connector in enumerate(self.connector_layers):
            horuglass_input = x
            x = self.hourglass_layers[i](x)  # input is passed through the Hourglass net
            x, heatmaps = connector(x, horuglass_input)  # then the input is passed to the connector_layer,
            # which outputs the input for the next layer, and the heatmap predictions
            hg_output.append(heatmaps)
        x = self.hourglass_layers[-1](x)  # the output is passed to the last layer
        hg_output.append(x)
        return hg_output  # returning all the outputted heatmaps, as loss is calculated on each of them('immediate supervision')

'''Dataset(DATASET_PATH, LABEL_PATH,
        transform=torchvision.transforms.Compose([resize_transform, scale_transform]),
        label_transform=torchvision.transforms.Compose([coordinate_to_heatmap_transform]))'''

"""##Trainer"""

import numpy as np
import torch


class Trainer:
    def __init__(self, model, criterion, optimizer, config, scheduler=None):
        self.model = model
        self.criterion = criterion
        self.optimizer = optimizer
        self.loss = {"train": [], "val": []}
        self.epochs = config["epochs"]
        self.batches_per_epoch = config["batches_per_epoch"]
        self.batches_per_epoch_val = config["batches_per_epoch_val"]
        self.device = config["device"]
        self.scheduler = scheduler
        self.checkpoint_frequency = 20
        self.early_stopping_epochs = 10
        self.early_stopping_avg = 10
        self.early_stopping_precision = 5

    def train(self, train_dataloader, val_dataloader=None):
      # 'val_dataloader' has a default None value only for this version, where we dont use 
      # validation set  
        for epoch in range(self.epochs):
            self._epoch_train(train_dataloader)
            self._epoch_eval(val_dataloader)
            print("Train loss - ", np.round(self.loss["train"][-1], 10))
            print(
                "Epoch: {}/{}, Train Loss={} Valid Loss={}".format(
                    epoch + 1,
                    self.epochs,
                    np.round(self.loss["train"][-1], 10),
                    np.round(self.loss["val"][-1], 10),
                )
            )

            # reducing LR if no improvement
            if self.scheduler is not None:
                self.scheduler.step(self.loss["train"][-1])

            # saving model
            if (epoch + 1) % self.checkpoint_frequency == 0:
                torch.save(
                    self.model.state_dict(), os.path.join(DATASET_ROOT_DIR_PATH, "hourglass_model_built_from_existing.pth")
                )

            # early stopping
            if epoch < self.early_stopping_avg:
                min_val_loss = np.round(np.mean(self.loss["val"]), self.early_stopping_precision)
                no_decrease_epochs = 0

            else:
                
                val_loss = np.round(
                    np.mean(self.loss["val"][-self.early_stopping_avg:]), 
                                    self.early_stopping_precision
                )
                if val_loss >= min_val_loss:
                    no_decrease_epochs += 1
                else:
                    min_val_loss = val_loss
                    no_decrease_epochs = 0
                    #print('New min: ', min_val_loss)

            if no_decrease_epochs > self.early_stopping_epochs:
                print("Early Stopping")
                break

        torch.save(self.model.state_dict(), os.path.join(DATASET_ROOT_DIR_PATH, "model_final"))
        return self.model

    def _epoch_train(self, dataloader):
        self.model.train()
        running_loss = []

        for i, data in enumerate(dataloader, 0):
            
            #inputs = data["image"].to(self.device)
            #labels = data["heatmaps"].to(self.device)
            inputs, labels = data  # updated to fit our Dataset
            inputs = inputs.to(self.device)
            labels = labels.to(self.device)

            inputs = inputs.permute(0, 3, 1, 2)

            self.optimizer.zero_grad()

            outputs = self.model(inputs)
            loss = self.criterion(outputs, labels)
            loss.backward()
            self.optimizer.step()

            running_loss.append(loss.item())

            if i == self.batches_per_epoch:
                epoch_loss = np.mean(running_loss)
                self.loss["train"].append(epoch_loss)
                print("finished_epoch")
                break

    def _epoch_eval(self, dataloader):
        self.model.eval()
        running_loss = []

        with torch.no_grad():
            for i, data in enumerate(dataloader, 0):
                inputs, labels = data
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                #inputs = data["image"].to(self.device)
                #labels = data["heatmaps"].to(self.device)
                inputs = inputs.permute(0, 3, 1, 2)

                outputs = self.model(inputs)
                loss = self.criterion(outputs, labels)

                running_loss.append(loss.item())

                if i == self.batches_per_epoch_val:
                    epoch_loss = np.mean(running_loss)
                    self.loss["val"].append(epoch_loss)
                    break

"""### Train Parameters"""

config = {
    "data_dir": DATASET_PATH,
    "epochs": 1000,
    "batch_size": 48,
    "batches_per_epoch": 50,
    "batches_per_epoch_val": 20,
    "learning_rate": 0.1,
    "device": torch.device("cuda" if torch.cuda.is_available() else "cpu"),
}

"""####Loading the Model

"""

# Loader

class Loader:
    def __init__(self, filename='/content/checkpoint.pth'):
        self._filename = filename

    def save_checkpoint(self, model, optimizer, lr_sched, epoch=None):
        checkpoint = {
      'epoch': epoch,
      'model': model.state_dict(),
      'optimizer': optimizer.state_dict(),
      'lr_sched': lr_sched}
        torch.save(checkpoint, self._filename)
      
    def save(self,model):
        torch.save(model.state_dict(), self._filename)

    def load_checkpoint(self):
        checkpoint = torch.load(self._filename)
        model = Model()
        print(model.load_state_dict(checkpoint['model']))        
        model = model.cuda() #  model should be moved to gpu before optimizers are created - 

        optimizer = torch.optim.Adam(model.parameters(), lr=INIT_LEARNING_RATE, weight_decay=WEIGHT_DECAY)
        optimizer.load_state_dict(checkpoint['optimizer'])
        
        '''scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',patience=5,factor=0.5,min_lr=1e-6,verbose=True)
        scheduler.load_state_dict(checkpoint['lr_sched'])
        checkpoint['lr_sched'] = scheduler'''

        try:  # this code throws an exception if optimizer.step() hasn't yet been called, which initializes the 
            #  lr_scheduler._last_lr member
          for g in optimizer.param_groups:
            g['lr'] = checkpoint['lr_sched']._last_lr[0]
        except:
          pass
        checkpoint['model'] = model
        checkpoint['optimizer'] = optimizer

        return checkpoint

    def load(self):
        model = ShallowUNet(N_IMG_CHANNELS, N_KEYPOINTS)
        model.load_state_dict(torch.load(self._filename))
        return model
    def load_hourglass(self):
      model = Hourglass(extract_heatmaps=True)
      model.load_state_dict(torch.load(self._filename))
      return model

"""## Data"""

import torchvision
from torch.utils.data import random_split

print(DATASET_PATH, LABEL_PATH)
train_dataset = Dataset(DATASET_PATH, LABEL_PATH,
        transform=torchvision.transforms.Compose([resize_transform, scale_transform]),
        label_transform=torchvision.transforms.Compose([coordinate_to_heatmap_transform]))

train, valid = random_split(train_dataset, [round(len(train_dataset) * 0.8), round(len(train_dataset)*0.2)])


train_dataloader = torch.utils.data.DataLoader(
    train, config["batch_size"], shuffle=True, drop_last=True, num_workers=2
)

'''val_dataset = FreiHAND(config=config, set_type="val")
'''
val_dataloader = torch.utils.data.DataLoader(
    valid, config["batch_size"], shuffle=True, drop_last=True, num_workers=2
)
pass

# visualize random batch of data train samples + labels
show_data(train_dataset, n_samples=8)

"""## Model"""

#model = ShallowUNet(N_IMG_CHANNELS, N_KEYPOINTS)
loader = Loader(os.path.join(DATASET_ROOT_DIR_PATH, "model_built_from_existing.pth"))
model = loader.load()

model = model.to(config["device"])

criterion = IoULoss()
optimizer = torch.optim.SGD(model.parameters(), lr=config["learning_rate"])
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer=optimizer, factor=0.5, patience=20, verbose=True, threshold=0.00001
)

"""##Our Model

"""

model = Hourglass(extract_heatmaps=True)
'''loader = Loader(os.path.join(DATASET_ROOT_DIR_PATH, "hourglass_model_built_from_existing.pth"))
model = loader.load_hourglass()'''

model = model.to(config["device"])

criterion = IoULoss()
optimizer = torch.optim.SGD(model.parameters(), lr=config["learning_rate"])
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer=optimizer, factor=0.5, patience=20, verbose=True, threshold=0.00001
)

"""## Training"""

trainer = Trainer(model, criterion, optimizer, config, scheduler)
model = trainer.train(train_dataloader, val_dataloader)

plt.plot(trainer.loss["train"], label="train")
plt.plot(trainer.loss["val"], label="val")
plt.legend()
plt.show()

""".

.

.

.

.

.

#Testing

###Model
"""

model = ShallowUNet(3, 21)
model.load_state_dict(
    torch.load(config["model_path"], map_location=torch.device(config["device"]))
)
model.eval()
print("Model loaded")

"""## Evaluate on Test Data"""

accuracy_all = []

for data in tqdm(test_dataloader):
    inputs = data["image"]
    pred_heatmaps = model(inputs)
    pred_heatmaps = pred_heatmaps.detach().numpy()
    true_keypoints = data["keypoints"].numpy()
    pred_keypoints = heatmaps_to_coordinates(pred_heatmaps)

    accuracy_keypoint = ((true_keypoints - pred_keypoints) ** 2).sum(axis=2) ** (1 / 2)
    accuracy_image = accuracy_keypoint.mean(axis=1)
    accuracy_all.extend(list(accuracy_image))

error = np.mean(accuracy_all)
print("Average error per keypoint: {:.1f}% from image size".format(error * 100))

for img_size in [MODEL_IMG_SIZE, RAW_IMG_SIZE]:
    error_pixels = error * img_size
    image_size = f"{img_size}x{img_size}"
    print(
        "Average error per keypoint: {:.0f} pixels for image {}".format(
            error_pixels, image_size
        )
    )

"""## Inference on Test Data"""

for data in test_dataloader:
    show_batch_predictions(data, model)
    break



"""##Model testing"""

import torch
import numpy as np
from copy import deepcopy
import cv2
import csv
import matplotlib.pyplot as plt
#plt.grid(True)
IMAGE_WIDTH = 128
HEATMAP_SIZE = 64
MAX_VALUE = 1
BLUR_GRID = (5, 5)
IMAGE_HEIGHT = IMAGE_WIDTH
DRAW_COLOR = (255, 0, 255)
RADIUS = 3
import sys


IMGS = ["2108.jpg", "740.png", "851.png", "1.jpg"]
IMG = IMGS[2]

labels = "1	0.36441788	0.445536455	0.447824874	0.457243591	0.512800227	0.431895386	0.566917673	0.408869238	0.633091056	0.365594954	0.551221099	0.490304527	0.620289607	0.512481795	0.667841676	0.530667445	0.721997155	0.537624592	0.534502177	0.5219684	0.589655445	0.555451339	0.630747183	0.584529666	0.680920323	0.603318752	0.480882826	0.543826178	0.527626565	0.569590243	0.560751094	0.595158293	0.600406114	0.615375169	0.43706942	0.559796205	0.458205032	0.581979868	0.477625865	0.600490337	0.499560939	0.607345543"
labels = labels.split("	") # the char in not spacebar!!
correct = [float(i) for i in labels]
correct_x = correct[1::2]
correct_y = correct[2::2]
correct_pairs = list(zip(correct_x, correct_y))


def parse_heatmaps(heatmaps):
    """
    Heatmaps is a numpy array
    Its size - (batch_size, n_keypoints, img_size, img_size)
    """
    batch_size = heatmaps.shape[0]
    sums = heatmaps.sum(axis=-1).sum(axis=-1)
    sums = np.expand_dims(sums, [2, 3])
    normalized = heatmaps / sums
    x_prob = normalized.sum(axis=2)
    y_prob = normalized.sum(axis=3)

    arr = np.tile(np.float32(np.arange(0, 64)), [batch_size, 21, 1])
    x = (arr * x_prob).sum(axis=2)
    y = (arr * y_prob).sum(axis=2)
    keypoints = np.stack([x, y], axis=-1)
    return keypoints / 64



def test2(model_path="model_built_from_existing.pth"):
    #model_path = os.path.join(DATASET_ROOT_DIR_PATH, "checkpoint (1).pth")
    
    
    heatmap_loss = IoULoss()
    image = cv2.imread(IMG)
    image = cv2.resize(image, (128,128))
    image = image / 255.0
    image = [image]
    image = torch.FloatTensor(image)
    image = image.permute(0, 3, 1, 2)

    loader = Loader(model_path)
    '''checkpoint = loader.load_checkpoint()
    #print(checkpoint)
    model = checkpoint['model']
    model = model.cpu()
    # the optimizer and loss we use are identical to those used in the AWR paper
    #optimizer = checkpoint['optimizer']
    #optimizer = optimizer.cpu()
    '''
    model = loader.load()
    #image = to_device(image, get_default_device())
    heatmaps = model(image)
    print(heatmaps.shape)
    
    heatmap = heatmaps#[-1]
    actual_labels = [float(i) for i in extract_labels(LABEL_PATH, int(IMG.split(".")[0]))]
    actual_labels = get_heatmaps(actual_labels)
    actual_labels = torch.FloatTensor(actual_labels)
    actual_labels = actual_labels.to(torch.float32)
    actual_labels = torch.unsqueeze(actual_labels, 0)
    print(actual_labels.shape, heatmap.shape)
    
    #actual_labels = to_device(actual_labels, get_default_device())
    '''loss = heatmap_loss(heatmaps, actual_labels)
    print("Model loss:", loss)'''
   

    heatmap = heatmap.detach().numpy()
    #labels = parse_heatmaps(heatmap)
    labels = [np.argmax(i) for i in heatmap]  #unsure why this was used
    labels = parse_heatmaps(heatmap)[0]
    points = [(round(i*500), round(j*500)) for i,j in labels]
    labels = [j for i in labels for j in i]
    print(labels)
    #points = [(int(500*((i//64)/64)), int(500*((i%64)/64))) for i in labels]  #unsure why this was used
    print(points)

    img = cv2.imread(IMG)
    image = cv2.resize(img, (500, 500), interpolation=cv2.INTER_AREA)
    correct_image = deepcopy(cv2.resize(img, (500, 500), interpolation=cv2.INTER_AREA))
    # labels = labels[0]
    # points = [[int(i[0] * 500), int(i[1] * 500)] for i in labels]
    actual_labels = [float(i) for i in extract_labels(LABEL_PATH, int(IMG.split(".")[0]))]
    print(actual_labels)
    actual_labels = zip(actual_labels[1::2], actual_labels[2::2])
    print(actual_labels)
    #correct_points = [(int(500*((i//64)/64)), int(500*((i%64)/64))) for i in actual_labels]
    correct_points = [(round(i*500), round(j*500)) for i,j in actual_labels]
    print(correct_points)
  

    for point in points:
        image = cv2.circle(image, point, RADIUS, DRAW_COLOR, cv2.FILLED)
        #print(point)

        #cv2_imshow(image)
    cv2_imshow(image)
    cv2.waitKey(0)

    for point in correct_points:
        correct_image = cv2.circle(correct_image, point, RADIUS, DRAW_COLOR, cv2.FILLED)
        #print(point)

    cv2_imshow(correct_image)
    cv2.waitKey(0)


def extract_labels(filename, idx):
      """

      :param filename:
      :return:
      """
      with open(filename, newline='') as f:
          reader = csv.reader(f)
          listed = list(reader)
      return listed[idx]


def test3(model_path="model_built_from_existing.pth"):
    print("model is successfully trained!!!!")
    #model_path = DATASET_ROOT_DIR_PATH, "single_image_model.pth")
    '''loader = Loader(model_path)
    checkpoint = loader.load_checkpoint()
    #print(checkpoint)
    model = checkpoint['model']
    model = model.cpu()'''
    # the optimizer and loss we use are identical to those used in the AWR paper
    #optimizer = checkpoint['optimizer']
    image = cv2.imread(IMG)

    #print(image)
    image = resize_transform(image)
    image = scale_transform(image)
    image = [image]
    image = torch.FloatTensor(image)
    image = image.permute(0, 3, 1, 2)
    loader = Loader(model_path)
    model = loader.load()
    heatmaps_model = model(image)[-1]
    heatmaps_model = heatmaps_model

    labels = [float(i) for i in extract_labels(LABEL_PATH, int(IMG.split(".")[0]))]
    our_heatmaps = np.asarray(get_heatmaps(labels))

    for i in range(len(our_heatmaps)):
        #print(heatmaps_model[0][i].shape, "max_ind:", np.argmax([np.argmax(row) for i, row in np.flip(heatmaps_model[0][i].detach().numpy())]))
        #print(heatmaps_model[0][i].detach().numpy().shape, np.argmax(heatmaps_model[0][i].detach().numpy(), axis=None))
        #print(np.argmax(heatmaps_model[0][i].detach().numpy())//64, np.argmax(heatmaps_model[0][i].detach().numpy())%64)
        #print(np.argmax(np.flip(heatmaps_model[0][i].detach().numpy(), 1))//64, np.argmax(np.flip(heatmaps_model[0][i].detach().numpy(),1))%64)

        """plt.imshow(np.flip(heatmaps_model[0][i].detach().numpy()))
        cv2.waitKey(0)
        plt.show()"""
  
        plt.imshow(heatmaps_model[0][i].detach().numpy())
        cv2.waitKey(0)
        plt.show()

        plt.imshow(our_heatmaps[i])
        plt.show()
        cv2.waitKey(0)


    labels = parse_heatmaps(heatmaps_model.detach().numpy())[0]
    points = [(round(i*500), round(j*500)) for i,j in labels]
    labels = [j for i in labels for j in i]
    print(labels)
    #points = [(int(500*((i//64)/64)), int(500*((i%64)/64))) for i in labels]  #unsure why this was used
    print(points)

    img = cv2.imread(IMG)
    image = cv2.resize(img, (500, 500), interpolation=cv2.INTER_AREA)
    correct_image = deepcopy(cv2.resize(img, (500, 500), interpolation=cv2.INTER_AREA))
    # labels = labels[0]
    # points = [[int(i[0] * 500), int(i[1] * 500)] for i in labels]
    actual_labels = [float(i) for i in extract_labels(LABEL_PATH, int(IMG.split(".")[0]))]
    print(actual_labels)
    actual_labels = zip(actual_labels[1::2], actual_labels[2::2])
    print(actual_labels)
    #correct_points = [(int(500*((i//64)/64)), int(500*((i%64)/64))) for i in actual_labels]
    correct_points = [(round(i*500), round(j*500)) for i,j in actual_labels]
    print(correct_points)
    
    print("reversedddddd keypoints:")
    reversed_image = deepcopy(image)
    for point in points:
        reversed_image = cv2.circle(reversed_image, point[::-1], RADIUS, DRAW_COLOR, cv2.FILLED)
        #print(point)

        #cv2_imshow(image)
        cv2_imshow(reversed_image)
        cv2.waitKey(0)
  


    print(parse_heatmaps(heatmaps_model.detach().numpy()))
    print(labels)

if __name__ == "__main__":
    
    print(os.path.join(DATASET_ROOT_DIR_PATH, "model_built_from_existing.pth"))
    #raise

    test2(os.path.join(DATASET_ROOT_DIR_PATH, "model_built_from_existing.pth"))
    '''if str(sys.argv[-1]) == "2":

        test2()
    else:
        test3()'''